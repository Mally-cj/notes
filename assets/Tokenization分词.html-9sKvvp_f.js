import{_ as e,c as t,a as n,o as i}from"./app-D5jitQEH.js";const l={};function a(s,o){return i(),t("div",null,o[0]||(o[0]=[n(`<h1 id="tokenization-分词" tabindex="-1"><a class="header-anchor" href="#tokenization-分词"><span>Tokenization 分词</span></a></h1><h2 id="前言" tabindex="-1"><a class="header-anchor" href="#前言"><span>前言</span></a></h2><p>2025-04-03 23:20:52 Thursday 参考博客 <a href="https://zhuanlan.zhihu.com/p/700283095?utm_psn=1891268230099760552" target="_blank" rel="noopener noreferrer">知乎博客</a></p><h2 id="分词介绍" tabindex="-1"><a class="header-anchor" href="#分词介绍"><span>分词介绍</span></a></h2><h3 id="tokenization-与-embedding-文本如何变成向量" tabindex="-1"><a class="header-anchor" href="#tokenization-与-embedding-文本如何变成向量"><span>Tokenization 与 Embedding：文本如何变成向量？</span></a></h3><p>一段文本输入给模型，转成 mebedding 会经过两个过程，</p><h4 id="_1-分词-tokenization" tabindex="-1"><a class="header-anchor" href="#_1-分词-tokenization"><span>1. <strong>分词（Tokenization）</strong></span></a></h4><ul><li><p><strong>目的</strong>：将原始文本拆分为模型可处理的离散单元（token），这些单元可能是单词、子词（subword）或字符。</p></li><li><p><strong>输出</strong>：生成一个由 token 组成的序列，每个 token 对应一个唯一的整数 ID（<code>token_id</code>）。</p></li><li><p><strong>例子</strong>：</p><ul><li>输入文本：<code>&quot;ChatGPT is powerful!&quot;</code></li><li>分词结果（以 OpenAI 的 tokenizer 为例）：<code>[&quot;Chat&quot;, &quot;G&quot;, &quot;PT&quot;, &quot; is&quot;, &quot; powerful&quot;, &quot;!&quot;]</code> → 对应的 <code>token_id</code> 可能是 <code>[1234, 567, 890, 11, 2345, 7]</code>。</li></ul></li></ul><h4 id="_2-token-到-embedding-的转换" tabindex="-1"><a class="header-anchor" href="#_2-token-到-embedding-的转换"><span>2. <strong>Token 到 Embedding 的转换</strong></span></a></h4><ul><li><p><strong>步骤</strong>：</p><ol><li><strong>Token ID 映射</strong>：每个 <code>token_id</code> 作为索引，在模型的 <strong>Embedding 矩阵</strong>（一个大小为 <code>[vocab_size, embedding_dim]</code> 的查找表）中查找对应的行。</li><li><strong>输出</strong>：得到该 token 的向量表示（embedding），维度为 <code>embedding_dim</code>（如 768、1024 等）。</li></ol></li><li><p><strong>关键点</strong>：</p><ul><li>Embedding 矩阵是模型训练时学习到的参数，每个 <code>token_id</code> 对应一个固定的向量。</li><li>同一 <code>token_id</code> 在不同模型中的 embedding 可能不同（因模型而异）。</li></ul></li></ul><h4 id="_3-流程总结" tabindex="-1"><a class="header-anchor" href="#_3-流程总结"><span>3. <strong>流程总结</strong></span></a></h4><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">原始文本 → Tokenization → [token_id1, token_id2, ...] → Embedding查找 → [embedding1, embedding2, ...]</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="分词的-3-种粒度" tabindex="-1"><a class="header-anchor" href="#分词的-3-种粒度"><span>分词的 3 种粒度</span></a></h3><p>在自然语言处理（NLP）中，分词（Tokenization）的粒度决定了如何将文本拆分为基本单元，主要分为 3 种力度：</p><ol><li><p><strong>词级别（Word-level）</strong></p><ul><li>按空格/标点切分，每个词作为独立 token（如英文：&quot;She is cute&quot; → <code>[&quot;She&quot;，&quot;is&quot;，&quot;cute&quot;]</code>）。</li><li><strong>缺点</strong>：词汇表大，难以处理未登录词（OOV）。</li></ul></li><li><p><strong>子词级别（Subword-level）</strong></p><ul><li>将词拆分为更小子单元（如 BPE 算法：&quot;She is cute&quot; → <code>[&quot;Sh&quot;, &quot;e&quot;, &quot;is&quot;,&quot;cu&quot;,&quot;te&quot;]</code>）。</li><li><strong>优点</strong>：平衡词汇量，解决 OOV 问题（主流模型如 BERT、GPT 均采用）。</li></ul></li><li><p><strong>字符级别（Character-level）</strong></p><ul><li>按字符切分（如：&quot;She is cute&quot; → <code>[&quot;S&quot;,&quot;h&quot;,&quot;e&quot;,&quot;i&quot;,&quot;s&quot;,&quot;c&quot;,&quot;u&quot;,&quot;t&quot;,&quot;e&quot;]</code>）。</li><li><strong>优点</strong>：词汇表极小；<strong>缺点</strong>：序列过长，训练效率低。</li></ul></li></ol><h3 id="提升分词效果的核心意义" tabindex="-1"><a class="header-anchor" href="#提升分词效果的核心意义"><span><strong>提升分词效果的核心意义</strong>：</span></a></h3><ol><li><p><strong>增强模型理解能力</strong></p><ul><li>更准确的分词（如合理拆分子词）能保留语义信息，帮助模型捕捉关键特征（例如将&quot;unhappy&quot;拆为 <code>[&quot;un&quot;, &quot;happy&quot;]</code> 以理解否定含义）。</li></ul></li><li><p><strong>解决未登录词（OOV）问题</strong></p><ul><li>优化分词策略（如子词切分）可减少生僻词、拼写变体的处理难度（如&quot;ChatGPT&quot;→<code>[&quot;Chat&quot;, &quot;G&quot;, &quot;PT&quot;]</code>）。</li></ul></li><li><p><strong>提升计算效率</strong></p><ul><li>平衡分词粒度（避免字符级过细或词级过粗）可缩短序列长度，降低计算开销。</li></ul></li><li><p><strong>适配多语言/特殊文本</strong></p><ul><li>针对中文、黏着语（如日语）或混合文本（如代码、网络用语）设计分词规则，扩展模型适用性。</li></ul></li></ol><p><strong>一句话总结</strong>：更好的分词 = 更准的语义理解 + 更强的泛化能力 + 更高的计算效率。</p><h2 id="分词算法" tabindex="-1"><a class="header-anchor" href="#分词算法"><span>分词算法</span></a></h2>`,19)]))}const r=e(l,[["render",a]]),u=JSON.parse('{"path":"/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B/Tokenization%E5%88%86%E8%AF%8D.html","title":"Tokenization 分词","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"前言","slug":"前言","link":"#前言","children":[]},{"level":2,"title":"分词介绍","slug":"分词介绍","link":"#分词介绍","children":[{"level":3,"title":"Tokenization 与 Embedding：文本如何变成向量？","slug":"tokenization-与-embedding-文本如何变成向量","link":"#tokenization-与-embedding-文本如何变成向量","children":[]},{"level":3,"title":"分词的 3 种粒度","slug":"分词的-3-种粒度","link":"#分词的-3-种粒度","children":[]},{"level":3,"title":"提升分词效果的核心意义：","slug":"提升分词效果的核心意义","link":"#提升分词效果的核心意义","children":[]}]},{"level":2,"title":"分词算法","slug":"分词算法","link":"#分词算法","children":[]}],"git":{},"filePathRelative":"AIGC相关/大模型/Tokenization分词.md"}');export{r as comp,u as data};
