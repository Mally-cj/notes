import{_ as o,c as r,a,b as t,e as d,o as l}from"./app-VxZ08IHQ.js";const i="/wiki/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/static/EGeBb5YQ5od2gsx3GHbcTXtPndx.png",s="/wiki/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/static/DqwubfeKWo5Xkyx1bdVcg1yonCb.png",n={},m={class:"MathJax",jax:"SVG",style:{position:"relative"}},p={style:{"vertical-align":"-0.023ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.061ex",height:"1.618ex",role:"img",focusable:"false",viewBox:"0 -705 469 715","aria-hidden":"true"},T={class:"MathJax",jax:"SVG",style:{position:"relative"}},h={style:{"vertical-align":"-0.464ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.348ex",height:"2.034ex",role:"img",focusable:"false",viewBox:"0 -694 596 899","aria-hidden":"true"};function b(Q,e){return l(),r("div",null,[e[7]||(e[7]=a('<h1 id="day2-rlhf-流程" tabindex="-1"><a class="header-anchor" href="#day2-rlhf-流程"><span>Day2 RLHF 流程</span></a></h1><p>参考</p><p><a href="https://blog.csdn.net/2401_84204413/article/details/145558035" target="_blank" rel="noopener noreferrer">5 分钟搞懂 RLHF:基础知识与完整流程(附实战案例)-CSDN 博客</a></p><p>面试题：</p><p>介绍下 RLHF 流程</p><h2 id="大模型的-rlhf-过程" tabindex="-1"><a class="header-anchor" href="#大模型的-rlhf-过程"><span>大模型的 RLHF 过程：</span></a></h2>',6)),t("ol",null,[t("li",null,[e[2]||(e[2]=d("预训练 +SFT 得到模型 A，作为 Policy model，参数为 ")),t("mjx-container",m,[(l(),r("svg",p,e[0]||(e[0]=[t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D703",d:"M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"})])])],-1)]))),e[1]||(e[1]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"θ")])],-1))])]),t("li",null,[e[5]||(e[5]=d("将模型 A 作为奖励模型 R（reward model）的初始值,用偏好数据来训练得到 R,参数为 ")),t("mjx-container",T,[(l(),r("svg",h,e[3]||(e[3]=[t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D719",d:"M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"})])])],-1)]))),e[4]||(e[4]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"ϕ")])],-1))])]),e[6]||(e[6]=t("li",null,"使用 PPO 算法优化模型 A。在优化过程中，除了要使用训练好的 R，还要使用另外两个模型，Reference Model 和 Value model（也叫 critic model）。",-1))]),e[8]||(e[8]=a('<table><tr><td><br></td><td>使用PPO时的初始值<br></td><td>PPO训练过程中状态<br></td><td>含义<br></td></tr><tr><td>POlicy model<br></td><td> $\\theta$<br></td><td><br></td><td><br></td></tr><tr><td>Reward model<br></td><td> $\\phi$<br></td><td>冻结<br></td><td>计算整个回答的收益<br></td></tr><tr><td>Reference Model<br></td><td> $\\theta$<br></td><td>冻结<br></td><td><br></td></tr><tr><td>Critic Model<br></td><td> $\\phi$<br></td><td><br></td><td>计算每个token的收益<br></td></tr></table><p><img src="'+i+'" alt=""></p><h2 id="那么-ppo-的算法的优化过程是怎么样的呢" tabindex="-1"><a class="header-anchor" href="#那么-ppo-的算法的优化过程是怎么样的呢"><span>那么 PPO 的算法的优化过程是怎么样的呢？</span></a></h2><p><img src="'+s+'" alt=""></p><ol><li>q 是问题（B 条 prompt），输入 Policy model（即 SFT 后的大模型），得到完整的回答 o（B<em>L，L 是序列长度），把 B</em>L 分别送入到 Reference model 和 Reward model，得到一个值 r，作为奖励，</li></ol><p>其中 PPO 算法中，还要</p>',6))])}const g=o(n,[["render",b]]),B=JSON.parse('{"path":"/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Day2%20RLHF%E6%B5%81%E7%A8%8B.html","title":"Day2 RLHF 流程","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"大模型的 RLHF 过程：","slug":"大模型的-rlhf-过程","link":"#大模型的-rlhf-过程","children":[]},{"level":2,"title":"那么 PPO 的算法的优化过程是怎么样的呢？","slug":"那么-ppo-的算法的优化过程是怎么样的呢","link":"#那么-ppo-的算法的优化过程是怎么样的呢","children":[]}],"git":{},"filePathRelative":"AIGC相关/大模型秋招/Day2 RLHF流程.md"}');export{g as comp,B as data};
