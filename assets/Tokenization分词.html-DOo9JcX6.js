import{_ as n,c as e,a as Q,b as t,e as a,o as T}from"./app-CiXouqGP.js";const l={},d={class:"MathJax",jax:"SVG",style:{position:"relative"}},r={style:{"vertical-align":"-1.238ex"},xmlns:"http://www.w3.org/2000/svg",width:"25.803ex",height:"3.607ex",role:"img",focusable:"false",viewBox:"0 -1047.1 11404.8 1594.2","aria-hidden":"true"};function s(i,o){return T(),e("div",null,[o[7]||(o[7]=Q(`<h1 id="tokenization-分词" tabindex="-1"><a class="header-anchor" href="#tokenization-分词"><span>Tokenization 分词</span></a></h1><h2 id="前言" tabindex="-1"><a class="header-anchor" href="#前言"><span>前言</span></a></h2><p>2025-04-03 23:20:52 Thursday 参考博客 <a href="https://zhuanlan.zhihu.com/p/700283095?utm_psn=1891268230099760552" target="_blank" rel="noopener noreferrer">知乎博客</a></p><h2 id="分词介绍" tabindex="-1"><a class="header-anchor" href="#分词介绍"><span>分词介绍</span></a></h2><h3 id="tokenization-与-embedding-文本如何变成向量" tabindex="-1"><a class="header-anchor" href="#tokenization-与-embedding-文本如何变成向量"><span>Tokenization 与 Embedding：文本如何变成向量？</span></a></h3><p>一段文本输入给模型，转成 mebedding 会经过两个过程，</p><h4 id="_1-分词-tokenization" tabindex="-1"><a class="header-anchor" href="#_1-分词-tokenization"><span>1. <strong>分词（Tokenization）</strong></span></a></h4><ul><li><p><strong>目的</strong>：将原始文本拆分为模型可处理的离散单元（token），这些单元可能是单词、子词（subword）或字符。</p></li><li><p><strong>输出</strong>：生成一个由 token 组成的序列，每个 token 对应一个唯一的整数 ID（<code>token_id</code>）。</p></li><li><p><strong>例子</strong>：</p><ul><li>输入文本：<code>&quot;ChatGPT is powerful!&quot;</code></li><li>分词结果（以 OpenAI 的 tokenizer 为例）：<code>[&quot;Chat&quot;, &quot;G&quot;, &quot;PT&quot;, &quot; is&quot;, &quot; powerful&quot;, &quot;!&quot;]</code> → 对应的 <code>token_id</code> 可能是 <code>[1234, 567, 890, 11, 2345, 7]</code>。</li></ul></li></ul><h4 id="_2-token-到-embedding-的转换" tabindex="-1"><a class="header-anchor" href="#_2-token-到-embedding-的转换"><span>2. <strong>Token 到 Embedding 的转换</strong></span></a></h4><ul><li><p><strong>步骤</strong>：</p><ol><li><strong>Token ID 映射</strong>：每个 <code>token_id</code> 作为索引，在模型的 <strong>Embedding 矩阵</strong>（一个大小为 <code>[vocab_size, embedding_dim]</code> 的查找表）中查找对应的行。</li><li><strong>输出</strong>：得到该 token 的向量表示（embedding），维度为 <code>embedding_dim</code>（如 768、1024 等）。</li></ol></li><li><p><strong>关键点</strong>：</p><ul><li>Embedding 矩阵是模型训练时学习到的参数，每个 <code>token_id</code> 对应一个固定的向量。</li><li>同一 <code>token_id</code> 在不同模型中的 embedding 可能不同（因模型而异）。</li></ul></li></ul><h4 id="_3-流程总结" tabindex="-1"><a class="header-anchor" href="#_3-流程总结"><span>3. <strong>流程总结</strong></span></a></h4><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">原始文本 → Tokenization → [token_id1, token_id2, ...] → Embedding查找 → [embedding1, embedding2, ...]</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="分词的-3-种粒度" tabindex="-1"><a class="header-anchor" href="#分词的-3-种粒度"><span>分词的 3 种粒度</span></a></h3><p>在自然语言处理（NLP）中，分词（Tokenization）的粒度决定了如何将文本拆分为基本单元，主要分为 3 种力度：</p><ol><li><p><strong>词级别（Word-level）</strong></p><ul><li>按空格/标点切分，每个词作为独立 token（如英文：&quot;She is cute&quot; → <code>[&quot;She&quot;，&quot;is&quot;，&quot;cute&quot;]</code>）。</li><li><strong>缺点</strong>：词汇表大，难以处理未登录词（OOV）。</li></ul></li><li><p><strong>子词级别（Subword-level）</strong></p><ul><li>将词拆分为更小子单元（如 BPE 算法：&quot;She is cute&quot; → <code>[&quot;Sh&quot;, &quot;e&quot;, &quot;is&quot;,&quot;cu&quot;,&quot;te&quot;]</code>）。</li><li><strong>优点</strong>：平衡词汇量，解决 OOV 问题（主流模型如 BERT、GPT 均采用）。</li></ul></li><li><p><strong>字符级别（Character-level）</strong></p><ul><li>按字符切分（如：&quot;She is cute&quot; → <code>[&quot;S&quot;,&quot;h&quot;,&quot;e&quot;,&quot;i&quot;,&quot;s&quot;,&quot;c&quot;,&quot;u&quot;,&quot;t&quot;,&quot;e&quot;]</code>）。</li><li><strong>优点</strong>：词汇表极小；<strong>缺点</strong>：序列过长，训练效率低。</li></ul></li></ol><h3 id="提升分词效果的核心意义" tabindex="-1"><a class="header-anchor" href="#提升分词效果的核心意义"><span><strong>提升分词效果的核心意义</strong>：</span></a></h3><ol><li><p><strong>增强模型理解能力</strong></p><ul><li>更准确的分词（如合理拆分子词）能保留语义信息，帮助模型捕捉关键特征（例如将&quot;unhappy&quot;拆为 <code>[&quot;un&quot;, &quot;happy&quot;]</code> 以理解否定含义）。</li></ul></li><li><p><strong>解决未登录词（OOV）问题</strong></p><ul><li>优化分词策略（如子词切分）可减少生僻词、拼写变体的处理难度（如&quot;ChatGPT&quot;→<code>[&quot;Chat&quot;, &quot;G&quot;, &quot;PT&quot;]</code>）。</li></ul></li><li><p><strong>提升计算效率</strong></p><ul><li>平衡分词粒度（避免字符级过细或词级过粗）可缩短序列长度，降低计算开销。</li></ul></li></ol><p><strong>一句话总结</strong>：更好的分词 = 更准的语义理解 + 更强的泛化能力 + 更高的计算效率。</p><h2 id="分词算法" tabindex="-1"><a class="header-anchor" href="#分词算法"><span>分词算法</span></a></h2><p>以下是几种常用的子词界别的分词方法</p><h3 id="bpe-byte-pair-encoding" tabindex="-1"><a class="header-anchor" href="#bpe-byte-pair-encoding"><span><strong>BPE（Byte Pair Encoding）</strong></span></a></h3><p>参考 <a href="https://zhuanlan.zhihu.com/p/448147465" target="_blank" rel="noopener noreferrer">BPE 图解算法</a></p><h4 id="算法流程" tabindex="-1"><a class="header-anchor" href="#算法流程"><span><strong>算法流程</strong></span></a></h4><ol><li><p><strong>初始化基础词表</strong></p><ul><li>将文本按<strong>字符级拆分</strong>（英文按字母，中文按单字），构成初始词表。</li><li><strong>示例</strong>：句子 <code>&quot;low lower&quot;</code> → 初始词表为 <code>{l, o, w, e, r}</code>，文本拆分为 <code>[&quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot; &quot;, &quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;]</code>。</li></ul></li><li><p><strong>统计相邻符号对频率</strong></p><ul><li>遍历语料，统计所有<strong>相邻符号对的共现次数</strong>（如 <code>(&quot;l&quot;, &quot;o&quot;)</code>、<code>(&quot;w&quot;, &quot; &quot;)</code>）。</li><li><strong>示例</strong>：在 <code>&quot;low lower&quot;</code> 中，<code>(&quot;l&quot;, &quot;o&quot;)</code> 出现 2 次，<code>(&quot;o&quot;, &quot;w&quot;)</code> 出现 2 次，<code>(&quot;w&quot;, &quot;e&quot;)</code> 出现 1 次。</li></ul></li><li><p><strong>合并最高频符号对</strong></p><ul><li>选择<strong>频率最高的符号对</strong>，合并为新子词，更新词表。</li><li><strong>示例</strong>：合并 <code>(&quot;l&quot;, &quot;o&quot;)</code> 为 <code>&quot;lo&quot;</code>，文本更新为 <code>[&quot;lo&quot;, &quot;w&quot;, &quot; &quot;, &quot;lo&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;]</code>，词表新增 <code>&quot;lo&quot;</code>。</li></ul></li><li><p><strong>迭代合并直至终止</strong></p><ul><li>重复步骤 2-3，直到达到预设的<strong>词表大小</strong>或<strong>无更高频符号对</strong>。</li><li><strong>示例</strong>：下一步合并 <code>(&quot;lo&quot;, &quot;w&quot;)</code> 为 <code>&quot;low&quot;</code>，最终词表包含 <code>&quot;low&quot;</code>、<code>&quot;er&quot;</code> 等子词，剩余文本拆分为 <code>[&quot;low&quot;, &quot; &quot;, &quot;low&quot;, &quot;er&quot;]</code>。</li></ul></li></ol><h4 id="缺点" tabindex="-1"><a class="header-anchor" href="#缺点"><span>缺点</span></a></h4><p>随着合并的次数增加，词表大小通常先增加后减小。迭代次数太小，大部分还是字母，没什么意义；迭代次数多，又重新变回了原来那几个词。所以词表大小要取一个中间值。</p><h3 id="wordpiece" tabindex="-1"><a class="header-anchor" href="#wordpiece"><span>WordPiece</span></a></h3><h4 id="wordpiece-vs-bpe-核心区别总结" tabindex="-1"><a class="header-anchor" href="#wordpiece-vs-bpe-核心区别总结"><span><strong>WordPiece vs BPE 核心区别总结</strong></span></a></h4><h5 id="_1-符号标记方式不同" tabindex="-1"><a class="header-anchor" href="#_1-符号标记方式不同"><span><strong>1. 符号标记方式不同</strong></span></a></h5><ul><li><p><strong>BPE</strong>（原始版）：</p><ul><li>词表里只有普通子词，比如 <code>&quot;ing&quot;</code>、<code>&quot;happy&quot;</code></li><li>例子：<code>&quot;playing&quot;</code> → 拆成 <code>[&quot;play&quot;, &quot;ing&quot;]</code></li></ul></li><li><p><strong>WordPiece</strong>（升级版）：</p><ul><li>词表里非词首的子词用 <code>##</code> 标记，比如 <code>&quot;##ing&quot;</code>、<code>&quot;##happy&quot;</code></li><li>例子：<code>&quot;playing&quot;</code> → 拆成 <code>[&quot;play&quot;, &quot;##ing&quot;]</code>（<code>##ing</code> 表示它是词的一部分）</li></ul></li></ul><h5 id="_2-合并策略不同" tabindex="-1"><a class="header-anchor" href="#_2-合并策略不同"><span><strong>2. 合并策略不同</strong></span></a></h5>`,31)),t("ul",null,[o[6]||(o[6]=Q("<li><p><strong>BPE</strong>：</p><ul><li><strong>谁出现多就合并谁</strong></li><li>例子：如果 <code>&quot;un&quot;+&quot;happy&quot;</code> 出现 100 次，<code>&quot;un&quot;+&quot;h&quot;</code> 出现 200 次 → 优先合并 <code>&quot;un&quot;+&quot;h&quot;</code> 变成 <code>&quot;unh&quot;</code></li></ul></li>",1)),t("li",null,[o[5]||(o[5]=t("p",null,[t("strong",null,"WordPiece"),a("：")],-1)),t("ul",null,[t("li",null,[o[2]||(o[2]=t("strong",null,'谁组合后更像一个"合理词"就合并谁，具体计算公式这样：',-1)),o[3]||(o[3]=a()),t("mjx-container",d,[(T(),e("svg",r,o[0]||(o[0]=[Q('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z" transform="translate(394,0)"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(838,0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1338,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1730,0)"></path></g><g data-mml-node="mo" transform="translate(2174,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2563,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(3313,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3757.7,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(4516.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5183.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(6239.2,0)"><g data-mml-node="mrow" transform="translate(1085.5,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(306,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(698,0)"></path><path data-c="71" d="M33 218Q33 308 95 374T236 441H246Q330 441 381 372L387 364Q388 364 404 403L420 442H457V156Q457 -132 458 -134Q462 -142 470 -145Q491 -148 519 -148H535V-194H527L504 -193Q480 -192 453 -192T415 -191Q312 -191 303 -194H295V-148H311Q339 -148 360 -145Q369 -141 371 -135T373 -106V-41V49Q313 -11 236 -11Q154 -11 94 53T33 218ZM376 300Q346 389 278 401Q275 401 269 401T261 402Q211 400 171 350T131 214Q131 137 165 82T253 27Q296 27 328 54T376 118V300Z" transform="translate(1142,0)"></path></g><g data-mml-node="mo" transform="translate(1670,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2059,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2809,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3087,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3846,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(306,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(698,0)"></path><path data-c="71" d="M33 218Q33 308 95 374T236 441H246Q330 441 381 372L387 364Q388 364 404 403L420 442H457V156Q457 -132 458 -134Q462 -142 470 -145Q491 -148 519 -148H535V-194H527L504 -193Q480 -192 453 -192T415 -191Q312 -191 303 -194H295V-148H311Q339 -148 360 -145Q369 -141 371 -135T373 -106V-41V49Q313 -11 236 -11Q154 -11 94 53T33 218ZM376 300Q346 389 278 401Q275 401 269 401T261 402Q211 400 171 350T131 214Q131 137 165 82T253 27Q296 27 328 54T376 118V300Z" transform="translate(1142,0)"></path></g><g data-mml-node="mo" transform="translate(1670,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2059,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2809,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3198,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(3476,0)"><path data-c="66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(306,0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(698,0)"></path><path data-c="71" d="M33 218Q33 308 95 374T236 441H246Q330 441 381 372L387 364Q388 364 404 403L420 442H457V156Q457 -132 458 -134Q462 -142 470 -145Q491 -148 519 -148H535V-194H527L504 -193Q480 -192 453 -192T415 -191Q312 -191 303 -194H295V-148H311Q339 -148 360 -145Q369 -141 371 -135T373 -106V-41V49Q313 -11 236 -11Q154 -11 94 53T33 218ZM376 300Q346 389 278 401Q275 401 269 401T261 402Q211 400 171 350T131 214Q131 137 165 82T253 27Q296 27 328 54T376 118V300Z" transform="translate(1142,0)"></path></g><g data-mml-node="mo" transform="translate(5146,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5535,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(6294,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="4925.6" height="60" x="120" y="220"></rect></g></g></g>',1)]))),o[1]||(o[1]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"score"),t("mo",{stretchy:"false"},"("),t("mi",null,"A"),t("mo",null,","),t("mi",null,"B"),t("mo",{stretchy:"false"},")"),t("mo",null,"="),t("mfrac",null,[t("mrow",null,[t("mi",null,"freq"),t("mo",{stretchy:"false"},"("),t("mi",null,"A"),t("mo",null,","),t("mi",null,"B"),t("mo",{stretchy:"false"},")")]),t("mrow",null,[t("mi",null,"freq"),t("mo",{stretchy:"false"},"("),t("mi",null,"A"),t("mo",{stretchy:"false"},")"),t("mo",null,"⋅"),t("mi",null,"freq"),t("mo",{stretchy:"false"},"("),t("mi",null,"B"),t("mo",{stretchy:"false"},")")])])])],-1))])]),o[4]||(o[4]=t("li",null,[a("例子：虽然 "),t("code",null,'"un"+"h"'),a(" 出现 200 次，但 "),t("code",null,'"un"+"happy"'),a(" 的互信息分数更高 → 合并为 "),t("code",null,'"unhappy"'),t("em",null,"（因为 100/(50 * 20) > 200/(50 * 500)，假设 freq(un)=50, freq(happy)=20, freq(h)=500）")],-1))])])])])}const c=n(l,[["render",s]]),p=JSON.parse('{"path":"/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Tokenization%E5%88%86%E8%AF%8D.html","title":"Tokenization 分词","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"前言","slug":"前言","link":"#前言","children":[]},{"level":2,"title":"分词介绍","slug":"分词介绍","link":"#分词介绍","children":[{"level":3,"title":"Tokenization 与 Embedding：文本如何变成向量？","slug":"tokenization-与-embedding-文本如何变成向量","link":"#tokenization-与-embedding-文本如何变成向量","children":[]},{"level":3,"title":"分词的 3 种粒度","slug":"分词的-3-种粒度","link":"#分词的-3-种粒度","children":[]},{"level":3,"title":"提升分词效果的核心意义：","slug":"提升分词效果的核心意义","link":"#提升分词效果的核心意义","children":[]}]},{"level":2,"title":"分词算法","slug":"分词算法","link":"#分词算法","children":[{"level":3,"title":"BPE（Byte Pair Encoding）","slug":"bpe-byte-pair-encoding","link":"#bpe-byte-pair-encoding","children":[]},{"level":3,"title":"WordPiece","slug":"wordpiece","link":"#wordpiece","children":[]}]}],"git":{},"filePathRelative":"AIGC相关/大模型秋招/Tokenization分词.md"}');export{c as comp,p as data};
