<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.20" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <link rel="icon" href="/favicon.ico"><meta name="keywords" content="mally, 技术笔记, 开发, 编程分享"><script>
          // var _hmt = _hmt || [];
          // (function() {
          //   var hm = document.createElement("script");
          //   hm.src = "https://hm.baidu.com/hm.js?xxxxxx";
          //   var s = document.getElementsByTagName("script")[0];
          //   s.parentNode.insertBefore(hm, s);
          // })();
        </script><title>Day2 RLHF 流程 | mally的技术笔记</title><meta name="description" content="贴心的编程学习路线，全面的编程知识百科">
    <link rel="preload" href="/assets/style-BrakXTOn.css" as="style"><link rel="stylesheet" href="/assets/style-BrakXTOn.css">
    <link rel="modulepreload" href="/assets/app-VxZ08IHQ.js"><link rel="modulepreload" href="/assets/Day2 RLHF流程.html-BZTH6FWL.js">
    <link rel="prefetch" href="/assets/index.html-BmmfldWy.js" as="script"><link rel="prefetch" href="/assets/关于作者.html-DB9IJC-L.js" as="script"><link rel="prefetch" href="/assets/index.html-LE6k81oZ.js" as="script"><link rel="prefetch" href="/assets/工具_软件包使用.html-CSsYnEia.js" as="script"><link rel="prefetch" href="/assets/首页.html-D6POMUSg.js" as="script"><link rel="prefetch" href="/assets/Day1 DPO公式推导.html-BOC93JFQ.js" as="script"><link rel="prefetch" href="/assets/Day3 SwiGLU激活函数.html-DlgMh5tI.js" as="script"><link rel="prefetch" href="/assets/Embedding 词嵌入 .html-D0cL-FTa.js" as="script"><link rel="prefetch" href="/assets/index.html-BDwUzVAs.js" as="script"><link rel="prefetch" href="/assets/Tokenization分词.html-BDsstSPZ.js" as="script"><link rel="prefetch" href="/assets/标准化技术.html-BoauoWai.js" as="script"><link rel="prefetch" href="/assets/404.html-C50LuJFK.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="/logo.jpg" alt="mally的技术笔记"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">mally的技术笔记</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/AIGC%E7%9B%B8%E5%85%B3/" aria-label="AIGC相关"><!--[--><!--[--><!--]--><!--]-->AIGC相关<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/%E5%85%B3%E4%BA%8E%E4%BD%9C%E8%80%85.html" aria-label="关于作者"><!--[--><!--[--><!--]--><!--]-->关于作者<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/AIGC%E7%9B%B8%E5%85%B3/" aria-label="AIGC相关"><!--[--><!--[--><!--]--><!--]-->AIGC相关<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/%E5%85%B3%E4%BA%8E%E4%BD%9C%E8%80%85.html" aria-label="关于作者"><!--[--><!--[--><!--]--><!--]-->关于作者<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><a class="route-link route-link-active auto-link vp-sidebar-item vp-sidebar-heading" href="/AIGC%E7%9B%B8%E5%85%B3/" aria-label="AIGC相关"><!--[--><!--[--><!--]--><!--]-->AIGC相关<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%B7%A5%E5%85%B7_%E8%BD%AF%E4%BB%B6%E5%8C%85%E4%BD%BF%E7%94%A8.html" aria-label="工具/软件包使用"><!--[--><!--[--><!--]--><!--]-->工具/软件包使用<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/AIGC%E7%9B%B8%E5%85%B3/%E9%A6%96%E9%A1%B5.html" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading active">大模型秋招 <!----></p><ul style="" class="vp-sidebar-children"><!--[--><li><a class="route-link route-link-active auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/" aria-label="大模型秋招"><!--[--><!--[--><!--]--><!--]-->大模型秋招<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Embedding%20%E8%AF%8D%E5%B5%8C%E5%85%A5%20.html" aria-label="Embedding 词嵌入"><!--[--><!--[--><!--]--><!--]-->Embedding 词嵌入<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/%E6%A0%87%E5%87%86%E5%8C%96%E6%8A%80%E6%9C%AF.html" aria-label="标准化技术"><!--[--><!--[--><!--]--><!--]-->标准化技术<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Day1%20DPO%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC.html" aria-label="Day1 DPO 公式推导"><!--[--><!--[--><!--]--><!--]-->Day1 DPO 公式推导<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Day3%20SwiGLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.html" aria-label="Day3 SwiGLU 激活函数"><!--[--><!--[--><!--]--><!--]-->Day3 SwiGLU 激活函数<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Tokenization%E5%88%86%E8%AF%8D.html" aria-label="Tokenization 分词"><!--[--><!--[--><!--]--><!--]-->Tokenization 分词<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link route-link-active auto-link vp-sidebar-item active" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Day2%20RLHF%E6%B5%81%E7%A8%8B.html" aria-label="Day2 RLHF 流程"><!--[--><!--[--><!--]--><!--]-->Day2 RLHF 流程<!--[--><!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div><h1 id="day2-rlhf-流程" tabindex="-1"><a class="header-anchor" href="#day2-rlhf-流程"><span>Day2 RLHF 流程</span></a></h1><p>参考</p><p><a href="https://blog.csdn.net/2401_84204413/article/details/145558035" target="_blank" rel="noopener noreferrer">5 分钟搞懂 RLHF:基础知识与完整流程(附实战案例)-CSDN 博客</a></p><p>面试题：</p><p>介绍下 RLHF 流程</p><h2 id="大模型的-rlhf-过程" tabindex="-1"><a class="header-anchor" href="#大模型的-rlhf-过程"><span>大模型的 RLHF 过程：</span></a></h2><ol><li>预训练 +SFT 得到模型 A，作为 Policy model，参数为 <mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>θ</mi></math></mjx-assistive-mml></mjx-container></li><li>将模型 A 作为奖励模型 R（reward model）的初始值,用偏好数据来训练得到 R,参数为 <mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.348ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 596 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D719" d="M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi></math></mjx-assistive-mml></mjx-container></li><li>使用 PPO 算法优化模型 A。在优化过程中，除了要使用训练好的 R，还要使用另外两个模型，Reference Model 和 Value model（也叫 critic model）。</li></ol><table><tr><td><br></td><td>使用PPO时的初始值<br></td><td>PPO训练过程中状态<br></td><td>含义<br></td></tr><tr><td>POlicy model<br></td><td> $\theta$<br></td><td><br></td><td><br></td></tr><tr><td>Reward model<br></td><td> $\phi$<br></td><td>冻结<br></td><td>计算整个回答的收益<br></td></tr><tr><td>Reference Model<br></td><td> $\theta$<br></td><td>冻结<br></td><td><br></td></tr><tr><td>Critic Model<br></td><td> $\phi$<br></td><td><br></td><td>计算每个token的收益<br></td></tr></table><p><img src="/wiki/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/static/EGeBb5YQ5od2gsx3GHbcTXtPndx.png" alt=""></p><h2 id="那么-ppo-的算法的优化过程是怎么样的呢" tabindex="-1"><a class="header-anchor" href="#那么-ppo-的算法的优化过程是怎么样的呢"><span>那么 PPO 的算法的优化过程是怎么样的呢？</span></a></h2><p><img src="/wiki/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/static/DqwubfeKWo5Xkyx1bdVcg1yonCb.png" alt=""></p><ol><li>q 是问题（B 条 prompt），输入 Policy model（即 SFT 后的大模型），得到完整的回答 o（B<em>L，L 是序列长度），把 B</em>L 分别送入到 Reference model 和 Reward model，得到一个值 r，作为奖励，</li></ol><p>其中 PPO 算法中，还要</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav" aria-label="page navigation"><a class="route-link auto-link prev" href="/AIGC%E7%9B%B8%E5%85%B3/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A7%8B%E6%8B%9B/Tokenization%E5%88%86%E8%AF%8D.html" aria-label="Tokenization 分词"><!--[--><div class="hint"><span class="arrow left"></span> Prev</div><div class="link"><span class="external-link">Tokenization 分词</span></div><!--]--></a><!----></nav><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-VxZ08IHQ.js" defer></script>
  </body>
</html>
